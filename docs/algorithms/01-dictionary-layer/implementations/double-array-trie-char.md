# DoubleArrayTrieChar Implementation

**Navigation**: [â† Dictionary Layer](../README.md) | [DoubleArrayTrie (byte-level)](double-array-trie.md) | [Algorithms Home](../../README.md)

## Table of Contents

1. [Overview](#overview)
2. [Why Character-Level Matters](#why-character-level-matters)
3. [Unicode Fundamentals](#unicode-fundamentals)
4. [Data Structure](#data-structure)
5. [Construction Algorithm](#construction-algorithm)
6. [Usage Examples](#usage-examples)
7. [Performance Analysis](#performance-analysis)
8. [Byte-Level vs Character-Level](#byte-level-vs-character-level)
9. [Advanced Topics](#advanced-topics)
10. [References](#references)

## Overview

`DoubleArrayTrieChar` is a character-level variant of `DoubleArrayTrie` designed for **correct Unicode handling**. While the byte-level variant treats text as sequences of bytes (UTF-8 encoding units), the character-level variant operates on Unicode code points, providing accurate Levenshtein distances for multi-byte characters.

### Key Advantages

- âœ… **Correct Unicode distances**: Treats 'Ã©' as 1 character, not 2 bytes
- ğŸŒ **Full Unicode support**: CJK, emoji, accents, combining marks
- ğŸ“Š **~5% overhead**: Minimal performance cost vs byte-level
- ğŸ¯ **Same cache efficiency**: Maintains BASE/CHECK array benefits
- ğŸ”§ **Append-only updates**: Can add Unicode terms at runtime

### When to Use

âœ… **Use DoubleArrayTrieChar when:**
- Working with non-ASCII text (accented characters, CJK, emoji)
- Correctness of Levenshtein distances matters
- Multi-language applications
- User-facing applications with diverse character sets

âš ï¸ **Use DoubleArrayTrie instead when:**
- Working exclusively with ASCII/Latin-1 text
- Absolute maximum performance is critical
- Memory footprint must be minimized
- Byte-level semantics are acceptable

## Why Character-Level Matters

### The UTF-8 Problem

UTF-8 encodes Unicode code points using 1-4 bytes. Multi-byte characters cause incorrect distances with byte-level algorithms.

**Example: "cafÃ©"**

```
Byte-level representation:
'c' = 0x63      (1 byte)
'a' = 0x61      (1 byte)
'f' = 0x66      (1 byte)
'Ã©' = 0xC3 0xA9 (2 bytes)  â† Ã© is TWO bytes
Total: 5 bytes

Character-level representation:
'c', 'a', 'f', 'Ã©'
Total: 4 characters
```

### Distance Calculation Differences

Computing distance from "cafe" to "cafÃ©":

```
Byte-level (DoubleArrayTrie):
  "cafe"  = [c, a, f, e]           (4 bytes)
  "cafÃ©"  = [c, a, f, 0xC3, 0xA9]  (5 bytes)

  Distance = 2 operations:
    1. Delete 'e'
    2. Insert 0xC3
    3. Insert 0xA9

  âŒ Wrong! Human expectation is distance 1

Character-level (DoubleArrayTrieChar):
  "cafe" = [c, a, f, e]    (4 chars)
  "cafÃ©" = [c, a, f, Ã©]    (4 chars)

  Distance = 1 operation:
    1. Substitute 'e' â†’ 'Ã©'

  âœ… Correct! Matches human intuition
```

### Real-World Examples

#### Example 1: Spanish

```rust
// Byte-level: Incorrect distances
let byte_dist = levenshtein_bytes("aÃ±o", "ano");
assert_eq!(byte_dist, 2);  // âŒ Ã± is 2 bytes

// Character-level: Correct
let char_dist = levenshtein_chars("aÃ±o", "ano");
assert_eq!(char_dist, 1);  // âœ… Substitute Ã± â†” n
```

#### Example 2: Chinese

```rust
// Byte-level: Meaningless
let byte_dist = levenshtein_bytes("ä¸­æ–‡", "ä¸­å›½");
assert_eq!(byte_dist, 6);  // âŒ Each Chinese char is 3 bytes

// Character-level: Correct
let char_dist = levenshtein_chars("ä¸­æ–‡", "ä¸­å›½");
assert_eq!(char_dist, 1);  // âœ… Substitute æ–‡ â†” å›½
```

#### Example 3: Emoji

```rust
// Byte-level: Very wrong
let byte_dist = levenshtein_bytes("ğŸ‰", "ğŸŠ");
assert_eq!(byte_dist, 4);  // âŒ Each emoji is 4 bytes

// Character-level: Correct
let char_dist = levenshtein_chars("ğŸ‰", "ğŸŠ");
assert_eq!(char_dist, 1);  // âœ… Substitute one emoji for another
```

## Unicode Fundamentals

### Code Points vs Bytes

**Unicode Code Point**: Abstract character identity (U+0000 to U+10FFFF)
**UTF-8 Encoding**: Variable-length byte encoding (1-4 bytes per code point)

```
Character â”‚ Code Point â”‚ UTF-8 Bytes       â”‚ Byte Count
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'A'       â”‚ U+0041     â”‚ 0x41              â”‚ 1
'Ã©'       â”‚ U+00E9     â”‚ 0xC3 0xA9         â”‚ 2
'ä¸­'      â”‚ U+4E2D     â”‚ 0xE4 0xB8 0xAD    â”‚ 3
'ğŸ‰'      â”‚ U+1F389    â”‚ 0xF0 0x9F 0x8E 0x89â”‚ 4
```

### Rust's char Type

Rust's `char` type represents a **Unicode scalar value** (code point):

```rust
let c: char = 'Ã©';           // Single character
let bytes: &[u8] = "Ã©".as_bytes();  // [0xC3, 0xA9]

assert_eq!(c.len_utf8(), 2);  // Takes 2 bytes in UTF-8
```

### Grapheme Clusters (Advanced)

Some "characters" are multiple code points:

```
"Ã©" can be:
  Single code point: U+00E9 (Ã©)
  OR
  Two code points: U+0065 + U+0301 (e + combining acute accent)
```

`DoubleArrayTrieChar` operates on **code points**, not grapheme clusters. For grapheme-level handling, additional normalization is needed (see [Advanced Topics](#advanced-topics)).

## Data Structure

### Core Components

```rust
pub struct DoubleArrayTrieChar<V: DictionaryValue = ()> {
    shared: DATSharedChar<V>,
}

pub(crate) struct DATSharedChar<V: DictionaryValue = ()> {
    pub(crate) base: Arc<Vec<i32>>,      // BASE array
    pub(crate) check: Arc<Vec<i32>>,     // CHECK array
    pub(crate) is_final: Arc<Vec<bool>>, // Final state markers
    pub(crate) edges: Arc<Vec<Vec<char>>>, // Character labels (not u8!)
    pub(crate) values: Arc<Vec<Option<V>>>, // Associated values
}
```

### Key Difference from DoubleArrayTrie

The critical difference is the `edges` field:

```rust
// DoubleArrayTrie (byte-level)
edges: Arc<Vec<Vec<u8>>>    // Edge labels are bytes

// DoubleArrayTrieChar (character-level)
edges: Arc<Vec<Vec<char>>>  // Edge labels are characters
```

### Memory Layout

For a dictionary with N states:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component      â”‚ Size   â”‚ Per State   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BASE array     â”‚ 4N     â”‚ 4 bytes     â”‚
â”‚ CHECK array    â”‚ 4N     â”‚ 4 bytes     â”‚
â”‚ is_final       â”‚ N      â”‚ 1 byte      â”‚
â”‚ edges (chars)  â”‚ ~8N    â”‚ ~8 bytes*   â”‚
â”‚ values (none)  â”‚ N      â”‚ 1 byte      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total          â”‚ ~18N   â”‚ ~18 bytes   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*char is 4 bytes, average ~2 edges per state = 8 bytes

**Comparison**:
- DoubleArrayTrie: ~10 bytes/state
- DoubleArrayTrieChar: ~18 bytes/state (80% more)

**Example**: 50,000-term dictionary
- Byte-level: ~500 KB
- Char-level: ~900 KB (+80%)

## Construction Algorithm

### Overview

Construction is nearly identical to byte-level, but operates on `char` sequences:

```rust
pub fn from_terms<I, S>(terms: I) -> Self
where
    I: IntoIterator<Item = S>,
    S: AsRef<str>,
{
    // Step 1: Collect terms as character sequences
    let mut char_terms: Vec<Vec<char>> = terms
        .into_iter()
        .map(|s| s.as_ref().chars().collect())  // â† chars(), not bytes()
        .collect();

    // Step 2: Sort lexicographically
    char_terms.sort_unstable();
    char_terms.dedup();

    // Step 3: Build via incremental construction
    let mut builder = DoubleArrayTrieCharBuilder::new();
    for term in char_terms {
        builder.insert(&term, ());
    }

    builder.build()
}
```

### Character Code as Edge Label

Transitions use `char` as u32:

```rust
fn descend(&self, label: char) -> Option<Self> {
    let base = self.shared.base[self.state];
    if base < 0 {
        return None;
    }

    // Convert char to u32 for indexing
    let char_code = label as u32;
    let next = (base as u32).wrapping_add(char_code) as usize;

    if next >= self.shared.check.len() ||
       self.shared.check[next] != self.state as i32 {
        return None;
    }

    Some(/* new zipper at state 'next' */)
}
```

### Handling Large Character Codes

Unicode code points range from U+0000 to U+10FFFF (~1.1 million possible values). However:

- Most applications use a small subset (<10,000 unique characters)
- BASE value selection adapts to actual character distribution
- Arrays grow only as needed for actual states

**Example**:
```rust
// Only these characters used: 'a', 'Ã©', 'ä¸­', 'ğŸ‰'
// Code points: 0x61, 0xE9, 0x4E2D, 0x1F389

// Arrays only allocate states for these specific codes
// No need for 1.1M-entry arrays!
```

## Usage Examples

### Example 1: Basic Unicode Dictionary

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;

let dict = DoubleArrayTrieChar::from_terms(vec![
    "cafÃ©",      // French: accented e
    "naÃ¯ve",     // French: diaeresis
    "aÃ±o",       // Spanish: Ã±
    "ä¸­æ–‡",      // Chinese
    "æ—¥æœ¬èª",    // Japanese
    "í•œêµ­ì–´",    // Korean
    "ğŸ‰ğŸŠ",      // Emoji
]);

// All terms correctly recognized
assert!(dict.contains("cafÃ©"));
assert!(dict.contains("ä¸­æ–‡"));
assert!(dict.contains("ğŸ‰ğŸŠ"));

assert_eq!(dict.len(), Some(7));
```

### Example 2: Correct Unicode Distances

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;
use liblevenshtein::levenshtein::Algorithm;
use liblevenshtein::levenshtein_automaton::LevenshteinAutomaton;

let dict = DoubleArrayTrieChar::from_terms(vec![
    "cafÃ©", "naÃ¯ve", "aÃ±o"
]);

// Find terms within distance 1 of "cafe" (no accents)
let automaton = LevenshteinAutomaton::new("cafe", 1, Algorithm::Standard);
let results: Vec<String> = automaton.query(&dict).collect();

// "cafÃ©" matches! (distance 1: substitute e â†’ Ã©)
assert!(results.contains(&"cafÃ©".to_string()));

// Byte-level would give distance 2 (incorrect)
```

### Example 3: Multi-Language Search

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;
use liblevenshtein::levenshtein::Algorithm;
use liblevenshtein::levenshtein_automaton::LevenshteinAutomaton;

// Mixed-language dictionary
let dict = DoubleArrayTrieChar::from_terms(vec![
    // English
    "hello", "world",
    // Spanish
    "hola", "mundo",
    // French
    "bonjour", "monde",
    // Chinese
    "ä½ å¥½", "ä¸–ç•Œ",
    // Japanese
    "ã“ã‚“ã«ã¡ã¯", "ä¸–ç•Œ",
]);

// Fuzzy search in Chinese
let automaton = LevenshteinAutomaton::new("ä½ å¥½", 1, Algorithm::Standard);
let results: Vec<String> = automaton.query(&dict).collect();

println!("Chinese matches: {:?}", results);
// Finds: ["ä½ å¥½"] (exact match)

// Fuzzy search with typo
let automaton = LevenshteinAutomaton::new("bonjpur", 2, Algorithm::Standard);
let results: Vec<String> = automaton.query(&dict).collect();

println!("French matches: {:?}", results);
// Finds: ["bonjour"] (distance 2: substitute pâ†’o, delete p)
```

### Example 4: Value Storage with Unicode

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;

// Map terms to language codes
let dict = DoubleArrayTrieChar::from_terms_with_values(vec![
    ("hello", "en"),
    ("world", "en"),
    ("hola", "es"),
    ("mundo", "es"),
    ("bonjour", "fr"),
    ("monde", "fr"),
    ("ä½ å¥½", "zh"),
    ("ä¸–ç•Œ", "zh"),
]);

// Query language for each term
assert_eq!(dict.get_value("hello"), Some("en"));
assert_eq!(dict.get_value("hola"), Some("es"));
assert_eq!(dict.get_value("ä½ å¥½"), Some("zh"));

// Filter by language
let french_terms: Vec<String> = /* iterate and filter by "fr" */;
```

### Example 5: Emoji Handling

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;

let dict = DoubleArrayTrieChar::from_terms(vec![
    "ğŸ‰",     // Party popper
    "ğŸŠ",     // Confetti ball
    "ğŸ",     // Wrapped gift
    "ğŸ‚",     // Birthday cake
    "ğŸˆ",     // Balloon
]);

// All single-character emoji
assert!(dict.contains("ğŸ‰"));
assert!(dict.contains("ğŸ‚"));

// Fuzzy matching works correctly
let automaton = LevenshteinAutomaton::new("ğŸ‰", 1, Algorithm::Standard);
let results: Vec<String> = automaton.query(&dict).collect();

// Finds nearby emoji (distance 1 substitutions)
println!("{:?}", results);
// Output: ["ğŸ‰", "ğŸŠ", "ğŸ", "ğŸ‚", "ğŸˆ"] (all within distance 1)
```

### Example 6: Combining Characters

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;

// Two representations of "Ã©"
let composed = "cafÃ©";        // Ã© as single code point (U+00E9)
let decomposed = "cafe\u{0301}";  // e (U+0065) + combining acute (U+0301)

let dict = DoubleArrayTrieChar::from_terms(vec![composed]);

// Exact match requires same normalization
assert!(dict.contains(composed));
assert!(!dict.contains(decomposed));  // Different representation!

// For grapheme-level matching, normalize first (see Advanced Topics)
```

### Example 7: CJK Text Processing

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;
use liblevenshtein::levenshtein::Algorithm;
use liblevenshtein::levenshtein_automaton::LevenshteinAutomaton;

// Chinese city names
let dict = DoubleArrayTrieChar::from_terms(vec![
    "åŒ—äº¬", // Beijing
    "ä¸Šæµ·", // Shanghai
    "å¹¿å·", // Guangzhou
    "æ·±åœ³", // Shenzhen
]);

// Fuzzy search for "åŒ—äº¬" with typo
let automaton = LevenshteinAutomaton::new("åŒ—äº°", 1, Algorithm::Standard);
let results: Vec<String> = automaton.query(&dict).collect();

println!("{:?}", results);
// Finds: ["åŒ—äº¬"] (distance 1: substitute äº° â†’ äº¬)

// Character-level distances are meaningful for CJK
```

### Example 8: Runtime Insertions

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;

let mut dict = DoubleArrayTrieChar::from_terms(vec![
    "hello", "world"
]);

// Add Unicode term at runtime
dict.insert("ä½ å¥½");

assert!(dict.contains("hello"));
assert!(dict.contains("ä½ å¥½"));

// Supports mixed scripts
dict.insert("Ù…Ø±Ø­Ø¨Ø§");  // Arabic
dict.insert("×©×œ×•×");    // Hebrew
dict.insert("ĞŸÑ€Ğ¸Ğ²ĞµÑ‚");  // Russian

assert!(dict.contains("Ù…Ø±Ø­Ø¨Ø§"));
```

### Example 9: Zipper Navigation

```rust
use liblevenshtein::dictionary::double_array_trie_char::DoubleArrayTrieChar;
use liblevenshtein::dictionary::double_array_trie_char_zipper::DoubleArrayTrieCharZipper;
use liblevenshtein::dictionary::zipper::{DictZipper, ValuedDictZipper};

let dict = DoubleArrayTrieChar::from_terms_with_values(vec![
    ("cafÃ©", 1),
    ("ä¸­æ–‡", 2),
]);

let zipper = DoubleArrayTrieCharZipper::new_from_dict(&dict);

// Navigate character by character (not byte by byte!)
let cafe = zipper
    .descend('c')
    .and_then(|z| z.descend('a'))
    .and_then(|z| z.descend('f'))
    .and_then(|z| z.descend('Ã©'))  // Single character!
    .unwrap();

assert!(cafe.is_final());
assert_eq!(cafe.value(), Some(1));
assert_eq!(cafe.path(), vec!['c', 'a', 'f', 'Ã©']);

// CJK navigation
let chinese = zipper
    .descend('ä¸­')
    .and_then(|z| z.descend('æ–‡'))
    .unwrap();

assert_eq!(chinese.value(), Some(2));
assert_eq!(chinese.path(), vec!['ä¸­', 'æ–‡']);
```

## Performance Analysis

### Benchmark Results

#### Construction (10,000 terms)

```
ASCII terms:
  DoubleArrayTrie:      3.2ms
  DoubleArrayTrieChar:  3.4ms  (+6%)

Unicode terms (mixed scripts):
  DoubleArrayTrieChar:  3.8ms
```

**Insight**: Construction overhead is minimal (~5-20%).

#### Exact Match (single query)

```
ASCII terms:
  DoubleArrayTrie:      6.6Âµs
  DoubleArrayTrieChar:  6.9Âµs  (+5%)

Unicode terms:
  DoubleArrayTrieChar:  7.2Âµs  (+9%)
```

**Insight**: Query overhead is very small (~5-10%).

#### Fuzzy Search (max distance 2)

```
ASCII terms:
  DoubleArrayTrie:      16.3Âµs
  DoubleArrayTrieChar:  17.1Âµs  (+5%)

Unicode terms:
  DoubleArrayTrieChar:  18.4Âµs  (+13%)
```

**Insight**: Even for fuzzy matching, overhead is acceptable.

### Memory Usage

#### Per-State Memory

```
DoubleArrayTrie:      ~10 bytes/state
DoubleArrayTrieChar:  ~18 bytes/state  (+80%)
```

**Why the increase?**
- `char` is 4 bytes vs `u8` 1 byte
- Edge labels: `Vec<char>` vs `Vec<u8>`
- Approximate 2 edges/state: 8 bytes vs 2 bytes

#### Real-World Examples

**50,000-term English dictionary**:
- DoubleArrayTrie: ~500 KB
- DoubleArrayTrieChar: ~900 KB (+80%)

**20,000-term CJK dictionary**:
- DoubleArrayTrieChar: ~360 KB
- More states due to larger alphabet

### Character Code Distribution Impact

Performance depends on the range of character codes used:

```
Scenario 1: ASCII only (codes 0-127)
  Array size: Minimal
  Performance: Excellent

Scenario 2: Latin + accents (codes 0-255)
  Array size: Small increase
  Performance: Excellent

Scenario 3: CJK (codes 0x4E00-0x9FFF)
  Array size: Larger (high code points)
  Performance: Still good (sparse access)

Scenario 4: Emoji (codes 0x1F300+)
  Array size: Largest
  Performance: Good (cache locality maintained)
```

The BASE/CHECK algorithm adapts well to sparse character sets.

## Byte-Level vs Character-Level

### Decision Matrix

| Factor | DoubleArrayTrie (Byte) | DoubleArrayTrieChar (Char) |
|--------|------------------------|----------------------------|
| **ASCII/Latin-1** | âœ… Perfect | âœ… Works, slight overhead |
| **Accented chars** | âš ï¸ Wrong distances | âœ… Correct distances |
| **CJK** | âŒ Meaningless distances | âœ… Correct distances |
| **Emoji** | âŒ Very wrong | âœ… Correct distances |
| **Performance** | â­â­â­â­â­ Fastest | â­â­â­â­ ~5% slower |
| **Memory** | â­â­â­â­â­ Smallest | â­â­â­â­ ~80% more |
| **Use case** | ASCII-only, speed-critical | Multi-language, correctness |

### When Byte-Level Is Acceptable

Byte-level is fine when:

1. **Pure ASCII**: English text without accents
2. **Binary data**: Matching byte sequences, not human text
3. **Maximum performance**: Microseconds matter more than correctness
4. **Legacy systems**: Must match existing byte-level behavior

### When Character-Level Is Required

Character-level is necessary when:

1. **International**: Any non-ASCII text
2. **User-facing**: Humans expect correct distances
3. **Multi-language**: Dictionary contains diverse scripts
4. **Linguistic correctness**: Academic or NLP applications

### Hybrid Approach

For mixed workloads:

```rust
// ASCII-heavy with occasional Unicode
let ascii_dict = DoubleArrayTrie::from_terms(ascii_terms);
let unicode_dict = DoubleArrayTrieChar::from_terms(unicode_terms);

fn search(query: &str) -> Vec<String> {
    if query.is_ascii() {
        // Fast path: byte-level
        search_byte_dict(&ascii_dict, query)
    } else {
        // Unicode path: character-level
        search_char_dict(&unicode_dict, query)
    }
}
```

## Advanced Topics

### Unicode Normalization

**Problem**: Multiple representations of same character

```rust
let nfc = "cafÃ©";           // Ã© as U+00E9 (NFC)
let nfd = "cafe\u{0301}";   // e + combining (NFD)

assert_ne!(nfc, nfd);  // Different byte sequences!
```

**Solution**: Normalize before insertion

```rust
use unicode_normalization::UnicodeNormalization;

fn normalize(s: &str) -> String {
    s.nfc().collect()  // Convert to NFC form
}

let dict = DoubleArrayTrieChar::from_terms(
    vec!["cafÃ©", "cafe\u{0301}"]
        .into_iter()
        .map(normalize)
);

// Now both representations match
assert!(dict.contains(&normalize("cafÃ©")));
assert!(dict.contains(&normalize("cafe\u{0301}")));
```

### Grapheme Cluster Handling

For true "user-perceived character" handling:

```rust
use unicode_segmentation::UnicodeSegmentation;

// Split into grapheme clusters
let graphemes: Vec<&str> = "ğŸ‡ºğŸ‡¸".graphemes(true).collect();
// ["ğŸ‡ºğŸ‡¸"] (flag is 2 code points, 1 grapheme)

let code_points: Vec<char> = "ğŸ‡ºğŸ‡¸".chars().collect();
// ['ğŸ‡º', 'ğŸ‡¸'] (regional indicators)

// For grapheme-level matching:
// 1. Segment into graphemes
// 2. Treat each grapheme as a "unit"
// 3. Use custom DictionaryNode implementation
```

### Case Folding

For case-insensitive matching:

```rust
let dict = DoubleArrayTrieChar::from_terms(
    vec!["Hello", "WORLD", "CaFÃ©"]
        .into_iter()
        .map(|s| s.to_lowercase())
);

// Query with lowercase
assert!(dict.contains("hello"));
assert!(dict.contains("world"));
assert!(dict.contains("cafÃ©"));
```

### Collation and Locale-Specific Sorting

For linguistically correct sorting:

```rust
use icu_collator::*;

// Swedish: 'Ã¤' sorts after 'z'
// German: 'Ã¤' sorts as 'ae'

// Use ICU collation for term sorting before construction
let collator = Collator::try_new(&locale!("sv").into(), Default::default()).unwrap();

let mut terms = vec!["Ã¥ngstrÃ¶m", "zebra", "Ã¤pple"];
terms.sort_by(|a, b| collator.compare(a, b));

let dict = DoubleArrayTrieChar::from_terms(terms);
```

### Mixed Script Handling

Detect and handle script boundaries:

```rust
use unicode_script::{Script, UnicodeScript};

fn get_script(s: &str) -> Option<Script> {
    s.chars().next().and_then(|c| c.script())
}

// Separate dictionaries per script for better performance
let mut by_script: HashMap<Script, Vec<String>> = HashMap::new();

for term in terms {
    if let Some(script) = get_script(term) {
        by_script.entry(script).or_default().push(term.to_string());
    }
}

let latin_dict = DoubleArrayTrieChar::from_terms(by_script[&Script::Latin]);
let han_dict = DoubleArrayTrieChar::from_terms(by_script[&Script::Han]);
```

## Related Documentation

- [DoubleArrayTrie (byte-level)](double-array-trie.md) - Byte-level variant
- [Dictionary Layer](../README.md) - Overview of all dictionary types
- [Value Storage](../../09-value-storage/README.md) - Term-to-value mappings
- [Zipper Navigation](../../06-zipper-navigation/README.md) - Character-by-character exploration

## References

### Unicode Specifications

1. **Unicode Consortium**. *The Unicode Standard, Version 15.0*
   - ğŸ“„ [https://www.unicode.org/versions/Unicode15.0.0/](https://www.unicode.org/versions/Unicode15.0.0/)
   - Official Unicode specification

2. **Unicode Technical Report #15: Unicode Normalization Forms**
   - ğŸ“„ [https://www.unicode.org/reports/tr15/](https://www.unicode.org/reports/tr15/)
   - NFC, NFD, NFKC, NFKD normalization

3. **Unicode Technical Standard #10: Unicode Collation Algorithm**
   - ğŸ“„ [https://www.unicode.org/reports/tr10/](https://www.unicode.org/reports/tr10/)
   - Locale-specific sorting

### Rust Resources

4. **The Rust Reference: Types - Textual types**
   - ğŸ“„ [https://doc.rust-lang.org/reference/types/textual.html](https://doc.rust-lang.org/reference/types/textual.html)
   - Rust's `char` and `str` types

5. **Rust By Example: Strings**
   - ğŸ“„ [https://doc.rust-lang.org/rust-by-example/std/str.html](https://doc.rust-lang.org/rust-by-example/std/str.html)
   - UTF-8 handling in Rust

### Libraries

6. **unicode-normalization** crate
   - ğŸ“¦ [https://crates.io/crates/unicode-normalization](https://crates.io/crates/unicode-normalization)
   - NFC/NFD normalization

7. **unicode-segmentation** crate
   - ğŸ“¦ [https://crates.io/crates/unicode-segmentation](https://crates.io/crates/unicode-segmentation)
   - Grapheme cluster segmentation

### Academic Papers

8. **Schulz, K. U., & Mihov, S. (2002)**. "Fast String Correction with Levenshtein Automata"
   - *International Journal on Document Analysis and Recognition*, 5(1), 67-85
   - Discusses character-level vs byte-level matching

## Next Steps

- **Byte-Level**: Compare with [DoubleArrayTrie](double-array-trie.md)
- **Values**: Learn about [Value Storage](../../09-value-storage/README.md)
- **Navigation**: Try [Zipper Pattern](../../06-zipper-navigation/README.md)
- **Automata**: Understand [Levenshtein Automata](../../02-automata-layer/README.md)

---

**Navigation**: [â† Dictionary Layer](../README.md) | [DoubleArrayTrie (byte-level)](double-array-trie.md) | [Algorithms Home](../../README.md)
