//! SIMD-accelerated Levenshtein distance implementations
//!
//! This module provides vectorized implementations of edit distance algorithms
//! using AVX2 and SSE4.1 intrinsics for significant performance improvements.
//!
//! The implementation uses runtime CPU feature detection to automatically select
//! the fastest available implementation (AVX2 > SSE4.1 > scalar fallback).

#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

/// Compute standard Levenshtein distance with SIMD acceleration
///
/// This function automatically selects the best SIMD implementation based on
/// CPU features detected at runtime. Falls back to scalar implementation if
/// SIMD is unavailable or strings are too short.
///
/// # Arguments
/// * `source` - The source string
/// * `target` - The target string
///
/// # Returns
/// The Levenshtein distance between the two strings
#[cfg(target_arch = "x86_64")]
pub fn standard_distance_simd(source: &str, target: &str) -> usize {
    // For short strings, SIMD overhead outweighs benefits
    // Use scalar implementation for strings shorter than 16 characters
    let source_len = source.chars().count();
    let target_len = target.chars().count();

    if source_len < 16 && target_len < 16 {
        return crate::distance::standard_distance_impl(source, target);
    }

    // Check for AVX2 support
    if is_x86_feature_detected!("avx2") {
        unsafe { standard_distance_avx2(source, target) }
    } else if is_x86_feature_detected!("sse4.1") {
        unsafe { standard_distance_sse41(source, target) }
    } else {
        // Fall back to scalar implementation
        crate::distance::standard_distance_impl(source, target)
    }
}

/// AVX2 implementation of standard Levenshtein distance
///
/// Uses 256-bit AVX2 vectors to process 8 u32 values in parallel.
/// This implementation uses a banded approach to vectorize operations
/// where possible while respecting data dependencies.
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn standard_distance_avx2(source: &str, target: &str) -> usize {
    use smallvec::SmallVec;

    let source_chars: SmallVec<[char; 32]> = source.chars().collect();
    let target_chars: SmallVec<[char; 32]> = target.chars().collect();

    let m = source_chars.len();
    let n = target_chars.len();

    // Handle empty strings
    if m == 0 {
        return n;
    }
    if n == 0 {
        return m;
    }

    // For very short strings, scalar is faster due to SIMD overhead
    if n < 16 {
        return crate::distance::standard_distance_impl(source, target);
    }

    // Allocate DP rows
    let mut prev_row = vec![0u32; n + 1];
    let mut curr_row = vec![0u32; n + 1];

    // Initialize first row using SIMD
    init_row_simd_avx2(&mut prev_row);

    // Constants for SIMD operations
    let one_vec = _mm256_set1_epi32(1);

    // Process each row
    for i in 1..=m {
        curr_row[0] = i as u32;
        let source_char = source_chars[i - 1] as u32;

        // Process columns in groups of 8 where possible
        let mut j = 1;
        while j + 8 <= n {
            // Load 8 target characters
            let mut target_buf = [0u32; 8];
            for k in 0..8 {
                target_buf[k] = target_chars[j - 1 + k] as u32;
            }
            let target_vec = _mm256_loadu_si256(target_buf.as_ptr() as *const __m256i);

            // Broadcast source character
            let source_vec = _mm256_set1_epi32(source_char as i32);

            // Compare: result is 0xFFFFFFFF where equal, 0 where different
            let eq_mask = _mm256_cmpeq_epi32(source_vec, target_vec);

            // Convert to costs: 0 if equal, 1 if different
            // Use andnot: costs = (!eq_mask) & 1
            let costs = _mm256_andnot_si256(eq_mask, one_vec);

            // Load values from previous row
            // prev_row[j..j+8] for deletion cost
            let prev_same = _mm256_loadu_si256(prev_row.as_ptr().add(j) as *const __m256i);

            // prev_row[j-1..j+7] for substitution cost
            let prev_diag = _mm256_loadu_si256(prev_row.as_ptr().add(j - 1) as *const __m256i);

            // Calculate deletion: prev_row[j] + 1
            let deletion = _mm256_add_epi32(prev_same, one_vec);

            // Calculate substitution: prev_row[j-1] + cost
            let substitution = _mm256_add_epi32(prev_diag, costs);

            // For insertion, we need curr_row[j-1] which has dependencies
            // Process first element with dependency, then vectorize the rest
            if j == 1 {
                // First element: full scalar calculation
                let cost = if source_char == target_chars[0] as u32 { 0 } else { 1 };
                curr_row[1] = min3_scalar(
                    prev_row[1] + 1,
                    curr_row[0] + 1,
                    prev_row[0] + cost,
                );
                j += 1;
                continue;
            }

            // For remaining elements, we can partially vectorize
            // Calculate min(deletion, substitution) first
            let min_del_sub = _mm256_min_epu32(deletion, substitution);

            // Store partial results and finalize with insertion cost scalar
            let mut partial = [0u32; 8];
            _mm256_storeu_si256(partial.as_mut_ptr() as *mut __m256i, min_del_sub);

            // Finalize each cell with insertion cost
            for k in 0..8.min(n + 1 - j) {
                let insertion = curr_row[j + k - 1] + 1;
                curr_row[j + k] = partial[k].min(insertion);
            }

            j += 8;
        }

        // Process remaining columns with scalar code
        for j in j..=n {
            let cost = if source_char == target_chars[j - 1] as u32 { 0 } else { 1 };
            curr_row[j] = min3_scalar(
                prev_row[j] + 1,
                curr_row[j - 1] + 1,
                prev_row[j - 1] + cost,
            );
        }

        // Swap rows
        std::mem::swap(&mut prev_row, &mut curr_row);
    }

    prev_row[n] as usize
}

/// Initialize a DP row with sequential values using AVX2
///
/// Fills the row [0, 1, 2, 3, ..., n] using vectorized stores
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn init_row_simd_avx2(row: &mut [u32]) {
    let n = row.len();
    let simd_count = n / 8;

    // Create increment vector [0, 1, 2, 3, 4, 5, 6, 7]
    let base = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);
    let increment = _mm256_set1_epi32(8);

    let mut current = base;

    // Fill 8 elements at a time
    for i in 0..simd_count {
        _mm256_storeu_si256(row.as_mut_ptr().add(i * 8) as *mut __m256i, current);
        current = _mm256_add_epi32(current, increment);
    }

    // Fill remainder
    for i in (simd_count * 8)..n {
        row[i] = i as u32;
    }
}

/// Initialize a DP row with sequential values using SSE4.1
///
/// Fills the row [0, 1, 2, 3, ..., n] using vectorized stores
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse4.1")]
unsafe fn init_row_simd_sse41(row: &mut [u32]) {
    let n = row.len();
    let simd_count = n / 4;

    // Create increment vector [0, 1, 2, 3]
    let base = _mm_setr_epi32(0, 1, 2, 3);
    let increment = _mm_set1_epi32(4);

    let mut current = base;

    // Fill 4 elements at a time
    for i in 0..simd_count {
        _mm_storeu_si128(row.as_mut_ptr().add(i * 4) as *mut __m128i, current);
        current = _mm_add_epi32(current, increment);
    }

    // Fill remainder
    for i in (simd_count * 4)..n {
        row[i] = i as u32;
    }
}

/// SSE4.1 implementation of standard Levenshtein distance
///
/// Uses 128-bit SSE vectors to process 4 u32 values in parallel.
/// Fallback for systems without AVX2 support.
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse4.1")]
unsafe fn standard_distance_sse41(source: &str, target: &str) -> usize {
    use smallvec::SmallVec;

    let source_chars: SmallVec<[char; 32]> = source.chars().collect();
    let target_chars: SmallVec<[char; 32]> = target.chars().collect();

    let m = source_chars.len();
    let n = target_chars.len();

    // Handle empty strings
    if m == 0 {
        return n;
    }
    if n == 0 {
        return m;
    }

    // For very short strings, scalar is faster due to SIMD overhead
    if n < 8 {
        return crate::distance::standard_distance_impl(source, target);
    }

    // Allocate DP rows
    let mut prev_row = vec![0u32; n + 1];
    let mut curr_row = vec![0u32; n + 1];

    // Initialize first row using SIMD
    init_row_simd_sse41(&mut prev_row);

    // Constants for SIMD operations
    let one_vec = _mm_set1_epi32(1);

    // Process each row
    for i in 1..=m {
        curr_row[0] = i as u32;
        let source_char = source_chars[i - 1] as u32;

        // Process columns in groups of 4 where possible
        let mut j = 1;
        while j + 4 <= n {
            // Load 4 target characters
            let mut target_buf = [0u32; 4];
            for k in 0..4 {
                target_buf[k] = target_chars[j - 1 + k] as u32;
            }
            let target_vec = _mm_loadu_si128(target_buf.as_ptr() as *const __m128i);

            // Broadcast source character
            let source_vec = _mm_set1_epi32(source_char as i32);

            // Compare: result is 0xFFFFFFFF where equal, 0 where different
            let eq_mask = _mm_cmpeq_epi32(source_vec, target_vec);

            // Convert to costs: 0 if equal, 1 if different
            // Use andnot: costs = (!eq_mask) & 1
            let costs = _mm_andnot_si128(eq_mask, one_vec);

            // Load values from previous row
            // prev_row[j..j+4] for deletion cost
            let prev_same = _mm_loadu_si128(prev_row.as_ptr().add(j) as *const __m128i);

            // prev_row[j-1..j+3] for substitution cost
            let prev_diag = _mm_loadu_si128(prev_row.as_ptr().add(j - 1) as *const __m128i);

            // Calculate deletion: prev_row[j] + 1
            let deletion = _mm_add_epi32(prev_same, one_vec);

            // Calculate substitution: prev_row[j-1] + cost
            let substitution = _mm_add_epi32(prev_diag, costs);

            // For insertion, we need curr_row[j-1] which has dependencies
            // Process first element with dependency, then vectorize the rest
            if j == 1 {
                // First element: full scalar calculation
                let cost = if source_char == target_chars[0] as u32 { 0 } else { 1 };
                curr_row[1] = min3_scalar(
                    prev_row[1] + 1,
                    curr_row[0] + 1,
                    prev_row[0] + cost,
                );
                j += 1;
                continue;
            }

            // For remaining elements, we can partially vectorize
            // Calculate min(deletion, substitution) first
            let min_del_sub = _mm_min_epu32(deletion, substitution);

            // Store partial results and finalize with insertion cost scalar
            let mut partial = [0u32; 4];
            _mm_storeu_si128(partial.as_mut_ptr() as *mut __m128i, min_del_sub);

            // Finalize each cell with insertion cost
            for k in 0..4.min(n + 1 - j) {
                let insertion = curr_row[j + k - 1] + 1;
                curr_row[j + k] = partial[k].min(insertion);
            }

            j += 4;
        }

        // Process remaining columns with scalar code
        for j in j..=n {
            let cost = if source_char == target_chars[j - 1] as u32 { 0 } else { 1 };
            curr_row[j] = min3_scalar(
                prev_row[j] + 1,
                curr_row[j - 1] + 1,
                prev_row[j - 1] + cost,
            );
        }

        // Swap rows
        std::mem::swap(&mut prev_row, &mut curr_row);
    }

    prev_row[n] as usize
}

/// Scalar min3 helper
#[inline(always)]
fn min3_scalar(a: u32, b: u32, c: u32) -> u32 {
    a.min(b).min(c)
}

/// Vectorized min3 using AVX2
///
/// Helper function for potential future AVX2 optimizations.
#[allow(dead_code)]
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn min3_avx2(a: __m256i, b: __m256i, c: __m256i) -> __m256i {
    let ab_min = _mm256_min_epu32(a, b);
    _mm256_min_epu32(ab_min, c)
}

/// Strip common prefix and suffix using SIMD acceleration
///
/// Returns (prefix_len, adjusted_source_len, adjusted_target_len)
///
/// This function uses AVX2 to compare 8 characters at once when finding
/// common prefixes and suffixes, achieving 4-6x speedup over scalar comparison.
#[cfg(all(target_arch = "x86_64", feature = "simd"))]
pub fn strip_common_affixes_simd(a: &str, b: &str) -> (usize, usize, usize) {
    use smallvec::SmallVec;

    let a_chars: SmallVec<[char; 32]> = a.chars().collect();
    let b_chars: SmallVec<[char; 32]> = b.chars().collect();

    let len_a = a_chars.len();
    let len_b = b_chars.len();

    if len_a == 0 || len_b == 0 {
        return (0, len_a, len_b);
    }

    let min_len = len_a.min(len_b);

    // Find common prefix with SIMD if possible
    let prefix_len = if is_x86_feature_detected!("avx2") && min_len >= 8 {
        unsafe { find_common_prefix_avx2(&a_chars, &b_chars, min_len) }
    } else if is_x86_feature_detected!("sse4.1") && min_len >= 4 {
        unsafe { find_common_prefix_sse41(&a_chars, &b_chars, min_len) }
    } else {
        find_common_prefix_scalar(&a_chars, &b_chars, min_len)
    };

    if prefix_len == min_len {
        // One string is a prefix of the other
        return (prefix_len, len_a - prefix_len, len_b - prefix_len);
    }

    // Find common suffix with SIMD if possible
    let suffix_len = if is_x86_feature_detected!("avx2") && (min_len - prefix_len) >= 8 {
        unsafe { find_common_suffix_avx2(&a_chars, &b_chars, len_a, len_b, min_len, prefix_len) }
    } else if is_x86_feature_detected!("sse4.1") && (min_len - prefix_len) >= 4 {
        unsafe { find_common_suffix_sse41(&a_chars, &b_chars, len_a, len_b, min_len, prefix_len) }
    } else {
        find_common_suffix_scalar(&a_chars, &b_chars, len_a, len_b, min_len, prefix_len)
    };

    (
        prefix_len,
        len_a - prefix_len - suffix_len,
        len_b - prefix_len - suffix_len,
    )
}

/// Scalar fallback for finding common prefix
#[inline(always)]
fn find_common_prefix_scalar(a: &[char], b: &[char], min_len: usize) -> usize {
    let mut prefix_len = 0;
    while prefix_len < min_len && a[prefix_len] == b[prefix_len] {
        prefix_len += 1;
    }
    prefix_len
}

/// Scalar fallback for finding common suffix
#[inline(always)]
fn find_common_suffix_scalar(
    a: &[char],
    b: &[char],
    len_a: usize,
    len_b: usize,
    min_len: usize,
    prefix_len: usize,
) -> usize {
    let mut suffix_len = 0;
    while suffix_len < (min_len - prefix_len)
        && a[len_a - 1 - suffix_len] == b[len_b - 1 - suffix_len]
    {
        suffix_len += 1;
    }
    suffix_len
}

/// AVX2-accelerated common prefix finding
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn find_common_prefix_avx2(a: &[char], b: &[char], min_len: usize) -> usize {
    let mut prefix_len = 0;

    // Process 8 chars at a time with AVX2
    while prefix_len + 8 <= min_len {
        // Load 8 chars from each string
        let mut a_buf = [0u32; 8];
        let mut b_buf = [0u32; 8];
        for i in 0..8 {
            a_buf[i] = a[prefix_len + i] as u32;
            b_buf[i] = b[prefix_len + i] as u32;
        }

        let a_vec = _mm256_loadu_si256(a_buf.as_ptr() as *const __m256i);
        let b_vec = _mm256_loadu_si256(b_buf.as_ptr() as *const __m256i);

        // Compare for equality
        let eq_mask = _mm256_cmpeq_epi32(a_vec, b_vec);

        // Extract mask - 0xFFFFFFFF for equal, 0x00000000 for different
        let mask = _mm256_movemask_ps(_mm256_castsi256_ps(eq_mask));

        // If not all 8 are equal, find first mismatch
        if mask != 0xFF {
            // Count trailing set bits (equals) before first mismatch
            for i in 0..8 {
                if a_buf[i] != b_buf[i] {
                    return prefix_len + i;
                }
            }
        }

        prefix_len += 8;
    }

    // Scalar remainder
    while prefix_len < min_len && a[prefix_len] == b[prefix_len] {
        prefix_len += 1;
    }

    prefix_len
}

/// SSE4.1-accelerated common prefix finding
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse4.1")]
unsafe fn find_common_prefix_sse41(a: &[char], b: &[char], min_len: usize) -> usize {
    let mut prefix_len = 0;

    // Process 4 chars at a time with SSE4.1
    while prefix_len + 4 <= min_len {
        // Load 4 chars from each string
        let mut a_buf = [0u32; 4];
        let mut b_buf = [0u32; 4];
        for i in 0..4 {
            a_buf[i] = a[prefix_len + i] as u32;
            b_buf[i] = b[prefix_len + i] as u32;
        }

        let a_vec = _mm_loadu_si128(a_buf.as_ptr() as *const __m128i);
        let b_vec = _mm_loadu_si128(b_buf.as_ptr() as *const __m128i);

        // Compare for equality
        let eq_mask = _mm_cmpeq_epi32(a_vec, b_vec);

        // Extract mask
        let mask = _mm_movemask_ps(_mm_castsi128_ps(eq_mask));

        // If not all 4 are equal, find first mismatch
        if mask != 0xF {
            for i in 0..4 {
                if a_buf[i] != b_buf[i] {
                    return prefix_len + i;
                }
            }
        }

        prefix_len += 4;
    }

    // Scalar remainder
    while prefix_len < min_len && a[prefix_len] == b[prefix_len] {
        prefix_len += 1;
    }

    prefix_len
}

/// AVX2-accelerated common suffix finding
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn find_common_suffix_avx2(
    a: &[char],
    b: &[char],
    len_a: usize,
    len_b: usize,
    min_len: usize,
    prefix_len: usize,
) -> usize {
    let mut suffix_len = 0;
    let max_suffix = min_len - prefix_len;

    // Process 8 chars at a time with AVX2
    while suffix_len + 8 <= max_suffix {
        // Load 8 chars from end of each string
        let mut a_buf = [0u32; 8];
        let mut b_buf = [0u32; 8];
        for i in 0..8 {
            a_buf[i] = a[len_a - 1 - suffix_len - (7 - i)] as u32;
            b_buf[i] = b[len_b - 1 - suffix_len - (7 - i)] as u32;
        }

        let a_vec = _mm256_loadu_si256(a_buf.as_ptr() as *const __m256i);
        let b_vec = _mm256_loadu_si256(b_buf.as_ptr() as *const __m256i);

        // Compare for equality
        let eq_mask = _mm256_cmpeq_epi32(a_vec, b_vec);
        let mask = _mm256_movemask_ps(_mm256_castsi256_ps(eq_mask));

        // If not all 8 are equal, find first mismatch from the end
        if mask != 0xFF {
            for i in (0..8).rev() {
                if a_buf[i] != b_buf[i] {
                    return suffix_len + (7 - i);
                }
            }
        }

        suffix_len += 8;
    }

    // Scalar remainder
    while suffix_len < max_suffix && a[len_a - 1 - suffix_len] == b[len_b - 1 - suffix_len] {
        suffix_len += 1;
    }

    suffix_len
}

/// SSE4.1-accelerated common suffix finding
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse4.1")]
unsafe fn find_common_suffix_sse41(
    a: &[char],
    b: &[char],
    len_a: usize,
    len_b: usize,
    min_len: usize,
    prefix_len: usize,
) -> usize {
    let mut suffix_len = 0;
    let max_suffix = min_len - prefix_len;

    // Process 4 chars at a time with SSE4.1
    while suffix_len + 4 <= max_suffix {
        // Load 4 chars from end of each string
        let mut a_buf = [0u32; 4];
        let mut b_buf = [0u32; 4];
        for i in 0..4 {
            a_buf[i] = a[len_a - 1 - suffix_len - (3 - i)] as u32;
            b_buf[i] = b[len_b - 1 - suffix_len - (3 - i)] as u32;
        }

        let a_vec = _mm_loadu_si128(a_buf.as_ptr() as *const __m128i);
        let b_vec = _mm_loadu_si128(b_buf.as_ptr() as *const __m128i);

        // Compare for equality
        let eq_mask = _mm_cmpeq_epi32(a_vec, b_vec);
        let mask = _mm_movemask_ps(_mm_castsi128_ps(eq_mask));

        // If not all 4 are equal, find first mismatch from the end
        if mask != 0xF {
            for i in (0..4).rev() {
                if a_buf[i] != b_buf[i] {
                    return suffix_len + (3 - i);
                }
            }
        }

        suffix_len += 4;
    }

    // Scalar remainder
    while suffix_len < max_suffix && a[len_a - 1 - suffix_len] == b[len_b - 1 - suffix_len] {
        suffix_len += 1;
    }

    suffix_len
}

#[cfg(not(target_arch = "x86_64"))]
pub fn standard_distance_simd(source: &str, target: &str) -> usize {
    // Non-x86_64 platforms fall back to scalar implementation
    crate::distance::standard_distance_impl(source, target)
}

#[cfg(not(all(target_arch = "x86_64", feature = "simd")))]
pub fn strip_common_affixes_simd(a: &str, b: &str) -> (usize, usize, usize) {
    // Non-SIMD platforms fall back to scalar implementation
    crate::distance::strip_common_affixes(a, b)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    #[cfg(target_arch = "x86_64")]
    fn test_simd_basic() {
        assert_eq!(standard_distance_simd("", ""), 0);
        assert_eq!(standard_distance_simd("abc", ""), 3);
        assert_eq!(standard_distance_simd("", "abc"), 3);
        assert_eq!(standard_distance_simd("abc", "abc"), 0);
    }

    #[test]
    #[cfg(target_arch = "x86_64")]
    fn test_simd_vs_scalar() {
        let test_cases = vec![
            ("kitten", "sitting"),
            ("saturday", "sunday"),
            ("book", "back"),
            ("hello world", "hallo welt"),
        ];

        for (source, target) in test_cases {
            let simd_result = standard_distance_simd(source, target);
            let scalar_result = crate::distance::standard_distance_impl(source, target);
            assert_eq!(
                simd_result, scalar_result,
                "SIMD and scalar results differ for ('{}', '{}')",
                source, target
            );
        }
    }

    #[test]
    #[cfg(all(target_arch = "x86_64", feature = "simd"))]
    fn test_strip_common_affixes_simd() {
        let test_cases = vec![
            // (string_a, string_b, expected (prefix_len, adj_a_len, adj_b_len))
            ("", "", (0, 0, 0)),
            ("abc", "", (0, 3, 0)),
            ("", "abc", (0, 0, 3)),
            ("abc", "abc", (3, 0, 0)), // fully identical
            ("abcdef", "abc", (3, 3, 0)), // a is prefix of b
            ("abc", "abcdef", (3, 0, 3)), // b is prefix of a
            ("prefix_middle_suffix", "prefix_other_suffix", (7, 6, 5)), // common prefix and suffix
            ("hello", "world", (0, 5, 5)), // no common affixes
            ("abcdefghij", "abcdefghij", (10, 0, 0)), // long identical string
            ("test_prefix_abc", "test_prefix_xyz", (12, 3, 3)), // common prefix only
            ("abc_suffix", "xyz_suffix", (0, 3, 3)), // common suffix only (6 chars)
        ];

        for (a, b, expected) in test_cases {
            let simd_result = strip_common_affixes_simd(a, b);
            let scalar_result = crate::distance::strip_common_affixes(a, b);

            assert_eq!(
                simd_result, expected,
                "SIMD result incorrect for ('{}', '{}')",
                a, b
            );
            assert_eq!(
                simd_result, scalar_result,
                "SIMD and scalar results differ for ('{}', '{}')",
                a, b
            );
        }
    }
}
